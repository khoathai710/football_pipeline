# The data pipeline system for the Premier League 2025-2026

## 1. Introduction
This is a batch-processing data pipeline designed to collect weekly data for the 2025/2026 English Premier League season.
The system follows a typical ETL (Extract â€“ Transform â€“ Load) workflow.

What makes this system particularly unique is that its final stage is a Chatbot ðŸ¤– is **ScoutAI**. \
The Chatbot retrieves data from the data warehouse and interacts with users to answer their questions.


## 2. Design
### 2.1 Data pipeline
<img width="959" height="528" alt="Screenshot 2025-10-02 235535" src="https://github.com/user-attachments/assets/098deeab-06e6-4f9a-81a6-8287f82ab5b7" />

- We use Docker to containerize the application and Dagster to orchestrate assets.
- Data sources:  
  - Web: [premierleague.com](https://www.premierleague.com) for matches, match details, players, clubs, â€¦  
  - MySQL: for additional data about stadiums   
- All data from the first step is stored in MinIO as the data lake.  
- The loaded data from MinIO is then cleaned and transformed using Spark, producing silver and gold data.  
- The data is partitioned weekly (e.g., week_{}.parquet), since new matches and updates occur on a weekly basis.  
- The transformed data is loaded into PostgreSQL as the data warehouse and further modeled into data marts using dbt.  
- Finally, a Chatbot is created to query the data marts, allowing users to ask questions such as "Who is leading in goals?"
### 2.2 Project Structure
<img width="440" height="564" alt="Screenshot 2025-10-03 122929" src="https://github.com/user-attachments/assets/b30afebf-ea62-4a11-b03a-f6fbda6cc532" />

- **chatbot/**: Source code of the ScoutAI Chatbot.  
- **Dagster/**: Configurations and asset definitions for Dagster pipelines.  
- **dagster_home/**: Runtime files generated by Dagster (metadata, runs, logs).    
- **etl_pipeline/**: Core ETL logic (Extract, Transform, Load).  
- **football/**: dbt project for building data marts.    
- **Spark/**: Spark jobs and configurations for transformations.  

- **.env**: Environment variables (DB, MinIO, API keys, ...).  
- **docker-compose.yaml**: Docker Compose setup for running the full system.  
- **Makefile**: Utility commands for building and running the project.  
- **requirements.txt**: Python dependencies list.  

### 2.3 Datalake structure
<img width="411" height="655" alt="Screenshot 2025-10-03 122533" src="https://github.com/user-attachments/assets/6230e156-1b13-4dde-96ba-89e640f6d340" />


- The data lake is organized into three layers: **bronze**, **silver**, and **gold**.  
- All datasets are stored in **Parquet** format for efficient storage and querying.  
- Data is stored either as **full datasets** or in **weekly partitions** depending on update frequency.  

### 2.3 Data linage
#### 2.3.1 Global linage
<img width="919" height="355" alt="Screenshot 2025-10-03 001208" src="https://github.com/user-attachments/assets/72173be4-70bf-4b99-a825-81a5f72854c9" />

#### 2.3.2 Bronze layer
<img width="1686" height="465" alt="Screenshot 2025-10-03 001351" src="https://github.com/user-attachments/assets/506818cd-3e74-4110-b896-4ffd9be783d8" />


- **bronze_matches_dataset**: contains all information about matches such as goals, assists, cards, referees, substitutions, â€¦ and is materialized weekly  
- **bronze_football_stadiums_dataset**: contains information about the stadiums of each team and is materialized once  
- **bronze_player_info_dataset**: contains information about players (e.g., name, age, nationality, position, â€¦) and is materialized once  
- **bronze_teams_dataset**: contains information about teams (e.g., club name, manager, squad, â€¦) and is materialized once
#### 2.3.3 Silver layer
<img width="1740" height="378" alt="Screenshot 2025-10-03 002017" src="https://github.com/user-attachments/assets/d4fd33a1-3406-4845-9675-946ea059ca7e" />


- silver_goals_assists: extract and normalize goals & assists per match (weekly).  
- silver_cards: process yellow/red cards with time and player info (weekly).  
- silver_appearance: record player appearances (starting/subs) by match (weekly).  
- silver_player_info: clean player full name into first/last name (weekly).  
- silver_match_score: integrate match results with teams and man of the match (weekly).  
- silver_team_info: combine team info with stadium details (one-time).  

#### 2.3.4 Gold layer
<img width="1092" height="334" alt="Screenshot 2025-10-03 002413" src="https://github.com/user-attachments/assets/2ef6c205-1010-4e99-ac86-3f6a971f7e4e" />

- gold_goals_assists: integrate goals & assists with player details (weekly, Spark)  
- gold_cards: integrate card events with player details (weekly, Spark)  
- gold_appearance: clean and standardize player appearances (weekly, Pandas)  
- gold_player_info: standardized player information (one-time, Pandas)  
- gold_match_score: standardized match results with teams & scores (weekly, Pandas)  
- gold_team_info: standardized team information with stadium details (one-time, Pandas)

### 2.4 Containerize
<p float="left">
  <img width="45%" height="540" alt="Screenshot 2025-10-03 120407" src="https://github.com/user-attachments/assets/ecfd9d0f-1173-42e4-b058-0592b394149c" />
<img width="45%" height="666" alt="Screenshot 2025-10-03 120416" src="https://github.com/user-attachments/assets/6fbda0eb-141d-4303-b52a-2156aed528dd" />

</p>

**etl_pipeline**
- ports: none exposed  
- explanation: Contains the main ETL code, connected with Dagster to run pipelines.  

---
**de_mysql (MySQL)**
- ports: `3306:3306`  
- explanation: MySQL database for storing raw data.  
  Also mounts `./minio:/data` to communicate with MinIO.  

---
**de_psql (PostgreSQL)**
- ports: `5430:5432`  
- explanation: PostgreSQL database, usually used as a data warehouse or data mart for analytics.  

---
**de_dagster**
- ports: none exposed  
- explanation: Container building the Dagster image, where pipelines are executed.  

---
**de_dagster_dagit (Dagit UI)**  
- ports: `3001:3001`  
- explanation: Dagster web interface for monitoring pipelines, triggering jobs, and checking logs.  

---
**de_dagster_daemon (Dagster Daemon)**  
- ports: none exposed  
- explanation: Background process that manages Dagster jobs, schedules, and sensors.  

---
**minio (Object Storage, S3-like)**  
- ports: `9000:9000` (API), `9001:9001` (Web Console)  
- explanation: Object storage service (similar to Amazon S3). Used for storing raw, intermediate, or warehouse data.  

---
**mc (MinIO Client)**  
- ports: none exposed  
- explanation: CLI client for MinIO, used to create buckets (e.g., `warehouse`) and configure policies.  

---
**spark-master**  
- ports: `8080:8080` (Master UI), `4040:4040` (Application UI)  
- explanation: The master node of the Spark cluster, managing workers and job execution.  

---
**spark-worker**  
- ports: none exposed  
- explanation: Worker node in the Spark cluster, executes tasks assigned by the Spark Master.
## 3. Data Source
To retrieve information about a specific match, the pipeline follows these steps:

1. **Access the website** [premierleague.com](https://www.premierleague.com)  
   Welcome page:  
  <img width="1883" height="822" alt="Screenshot 2025-10-03 120926" src="https://github.com/user-attachments/assets/d9ca6589-44ad-470e-aa13-e4a6c631a0aa" />


2. **Navigate to the match page**  
   <img width="1885" height="828" alt="Screenshot 2025-10-03 121055" src="https://github.com/user-attachments/assets/f199f6c0-7cdf-45e7-ad1f-6641ef42c8c5" />


3. **Collect detailed information from multiple tabs** (e.g., *Recap*, *Lineups*, *Stats*)  
   <img width="1891" height="829" alt="Screenshot 2025-10-03 121542" src="https://github.com/user-attachments/assets/7df086f0-7ffd-4a4f-9e7a-7d48a6b5b7b9" />

4. All match data will be crawled using **Selenium**, extracted into **JSON format**, and then stored in **MinIO** for the next ETL steps.  
   For example, the following HTML snippet contains information about goals and assists:  

<img width="441" height="304" alt="Screenshot 2025-10-03 122108" src="https://github.com/user-attachments/assets/81530880-782a-4139-a428-ad9e6506b91a" />

  
## 4. Transformation with dbt
We use **SQL-based transformations** in the `models/` folder inside the `football` dbt project.  
To build the models, simply run:

```bash
dbt run
```

<img width="1046" height="545" alt="Screenshot 2025-10-03 123911" src="https://github.com/user-attachments/assets/09557e25-4edf-446e-b205-dde298a42099" />


## 5. Chatbot
The chatbot is designed to:  
- Answer questions about **Premier League teams, players, and matches**.  
- Provide quick access to **links, scores, and basic statistics**.  
- Support interaction with the **underlying database and warehouse** through natural language.  
- Help users explore insights from the **ETL & dbt pipeline** in a simple way.

<p float="left">
  <img width="45%" height="663" alt="Screenshot 2025-10-02 211158" 
       src="https://github.com/user-attachments/assets/8483aaf2-f360-4f3e-9b96-d3b08cc1123a" />
  <img width="45%" height="663" alt="Screenshot 2025-10-02 211210" 
       src="https://github.com/user-attachments/assets/249de9fb-8774-4a7a-85ef-2216d994252b" />
</p>

## 6. Acknowledgments

Thank you for reading this project!  
We truly appreciate your interest and support.
